{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Mining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make imports reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "DATA_DIR='../data'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/alex/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import brown dataset and stopwords from nltk\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import brown, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data from brown\n",
    "import re\n",
    "\n",
    "def preprocess_brown_sentence(sentence):\n",
    "    \"\"\"Preprocess data from brown\"\"\"\n",
    "    return ' '.join(list(filter(lambda x: re.match(r\"^[A-Za-z'\\-]+$\", x), sentence))).lower()\n",
    "\n",
    "brown_sentences_lowered = [preprocess_brown_sentence(sentence) for sentence in brown.sents()][:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substring Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique words from brown\n",
    "from mwe_discovery.phrase_mining.string_utils import get_words\n",
    "corpus = brown_sentences_lowered\n",
    "tokens = get_words(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the counts of words in brown\n",
    "min_support = 25\n",
    "token_counts = {token: 0 for token in tokens}\n",
    "\n",
    "for document in corpus:\n",
    "    words = document.split(' ')\n",
    "\n",
    "    for word in words:\n",
    "        if word in token_counts:\n",
    "            token_counts[word] += 1\n",
    "\n",
    "supported_tokens = [word for word in token_counts if token_counts[word]>=min_support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequent patterns in brown\n",
    "from mwe_discovery.phrase_mining.string_utils import text_pattern_checker, get_candidate_strings\n",
    "from mwe_discovery.phrase_mining.algorithms import a_priori\n",
    "pattern_checker = text_pattern_checker\n",
    "stop_tokens = set(stopwords.words('english'))\n",
    "\n",
    "frequent_patterns = a_priori(\n",
    "    corpus=corpus,\n",
    "    supported_tokens=supported_tokens,\n",
    "    stop_tokens=stop_tokens,\n",
    "    candidate_generator=get_candidate_strings,\n",
    "    pattern_checker=text_pattern_checker,\n",
    "    min_support=min_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['the', 'county', 'said', 'an', 'of', 'election', 'no', \"''\", 'that', 'in', 'city', 'committee', 'which', 'had', 'and', 'for', 'was', 'been', 'by', 'to', 'a', 'this', 'it', 'are', 'or', 'have', 'on', 'other', 'two', 'should', 'be', 'administration', 'is', 'as', 'also', 'at', 'state', 'one', 'program', 'but', 'has', 'with', 'they', 'we', 'some', 'will', 'its', 'from', 'new', 'not', 'there', 'plan', 'tax', 'his', 'more', 'than', 'year', 'home', 'council', 'he', 'who', 'after', 'would', 'party', 'up', 'out', '--', 'were', 'first', 'made', 'million', 'house', 'last', 'school', 'i', 'democratic', 'bill', 'their', 'if', 'president', 'states', 'government', 'united', 'the city', 'the state', 'he said', 'would be'])\n"
     ]
    }
   ],
   "source": [
    "# output the frequent patterns that we found\n",
    "print(frequent_patterns.keys())\n",
    "\n",
    "with open(f\"{DATA_DIR}/brown_phrases_new.txt\", 'w') as file:\n",
    "    file.write('\\n'.join(frequent_patterns.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phrase-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
